<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution | IEEE International Conference on Big Data 2024 </title> <meta name="author" content="ICBD Workshop"> <meta name="description" content="IEEE International Conference on Big Data 2024 Workshop "> <link rel="stylesheet" href="/icbd2024/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/icbd2024/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/icbd2024/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%95%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/icbd2024/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="/icbd2024/workshop/2019/coordconv/"> <script src="/icbd2024/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/icbd2024/"> IEEE International Conference on Big Data 2024 </a> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/icbd2024/cfp/">Call for Papers </a> </li> <li class="nav-item "> <a class="nav-link" href="/icbd2024/accepted_retrospectives/">Accepted Papers </a> </li> <li class="nav-item "> <a class="nav-link" href="/icbd2024/dates/">Important Dates </a> </li> <li class="nav-item "> <a class="nav-link" href="/icbd2024/schedule/">Schedule </a> </li> <li class="nav-item "> <a class="nav-link" href="/icbd2024/organizers/">Organizers </a> </li> </ul> </div> </div> </nav> </header> <div class="header-background"> <div class="img"></div> <div class="text"> <h2>Workshop on Big Data and AI for Healthcare @ ICBD 2024</h2> </div> </div> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A Retrospective for "An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution"</h1> <p class="post-meta">November 27, 2019</p> </header> <ul> <li>Original Paper : <a href="https://arxiv.org/abs/1807.03247" rel="external nofollow noopener" target="_blank">An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution</a> </li> <li>Paper written by : Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski</li> <li>Retrospective written by : Rosanne Liu, Joel Lehman, and Jason Yosinksi</li> </ul> <article class="post-content"> <h2 id="paper-tldr">Paper TL;DR</h2> <p>The main contribution of the paper was showing that a seemingly simple geometrical transformation was difficult for convolutional neural networks. A second contribution is a proposed fix, called CoordConv, which adds i,j pixel coordinates as additional inputs for a standard convolutional layer. Replacing standard convolution layers with CoordConv layers in several tasks (such as object detection, RL and generative models) improves performance.</p> <h2 id="an-experiment-in-interviews-for-retrospectives">An Experiment in Interviews for Retrospectives</h2> <p>One issue for retrospectives is the time and effort required to write them. Researchers are busy, and their incentives are oriented more towards writing new articles than reflecting on past ones. However, it can be fun to reminisce about old work, especially if someone is willing to indulgently listen. The authors of this retrospective (Rosanne, Joel, and Jason), at Jason’s suggestion, recorded a half-hour chat talking about a previous paper they collaborated on, and then transcribed and edited the interview for clarity and cohesiveness, which is shown below. We believe that interviews can potentially provide a relatively low-effort (especially if the product was audio without a polished transcript), and potentially fun way, to execute on retrospectives.</p> <h2 id="overall-outlook">Overall Outlook</h2> <h3 id="it-started-with-a-failure">It started with a failure</h3> <p>Joel: Rosanne, how did this get started – what was the project that led to CoordConv?</p> <p>Rosanne: It was something entirely different. Back then I was working on a project on emergent communication with Peter Dayan, and we wanted to study how agents establish communication schemes—with GANs!</p> <p>Jason: I remember telling you it was a bad idea because GANs don’t work.</p> <p>[Laughter]</p> <p>Rosanne: Long story short, the premise of that project was to get GANs to reliably produce objects with different shapes and colors, and have them move around – appear at random locations. If you know about the CLEVR dataset, we basically want GANs to generate that, without a rendering engine or anything.</p> <p>Jason: So your project started, and you said “let’s just figure out how to train GANs on moving objects with colors and shapes.”</p> <p>Rosanne: Yes. “How to make GANs work?” We’ve seen successful examples of GANs, but when you start working with them – especially on generating isolated objects moving about – it is very hard. You start to form this confusion between, am <em>I</em> not good enough at this, or are the <em>GANs</em> not good enough? This constant debate haunted me: is it my failure or is it the GAN’s failure?</p> <p>Jason: That was the point of the paper in the end, basically to prove that you weren’t a failure. I can’t believe it took us seven people and six months just to prop up your delicate ego.</p> <p>[Laughter]</p> <p>Jason: So you spent six months training GANs to draw squares, and the squares they drew weren’t quite squares.</p> <p>Rosanne: Well, actually, we started out more ambitious, aiming at the CLEVR dataset, with realistic 3D objects. That was hard, so we reduced it to sort-of-CLEVR, where objects are 2D. But there were still too many shapes and colors for GANs to work, so we further reduced it to two shapes and two colors, then one shape (squares) and black and white.</p> <p>Jason: So that was maybe three or five months in, and you were generating shapes and they were okay.</p> <p>Rosanne: Yes, but not in a way that was good enough for the project. GANs made decent shapes, maybe 1 out of 10 tries, but there were issues like mode collapse, and other things we were trying to fix.</p> <p>Jason: Right – things were oscillating. The discriminator was always winning and then always losing, always winning – back and forth and not converging. And you had no metric to judge quality – I think Peter was frustrated with that. Looking at the discriminator loss and generator loss, you can’t really tell whether the samples are good. It wasn’t clear what numbers you should be watching.</p> <p>Rosanne: Yeah – a lot of confusing things going on, and everyone’s frustrated, everyone’s sad.</p> <h3 id="debugging-the-failure">Debugging the Failure</h3> <p>Joel: So how did we get from there to CoordConv?</p> <p>Rosanne: Well then we started to think: why can’t GANs generate a good distribution of simple shapes – why can’t that simple concept work? We thought of all kinds of assumptions of where the model might break. We know having GANs draw shapes is easy – it paints faces, after all. So maybe it is the discreteness. Maybe at some point the network has to have discrete signals of <em>where</em> to draw the shapes and <em>what</em> shapes to draw. Since activations are continuous numbers throughout the layers of a neural network generator, maybe what we need is an architecture that takes the GAN’s noise vector and reduce it to a point where we sample discrete signals, and then generate image from those samples.</p> <p>Joel: That was on one of the whiteboard brainstorming images?</p> <p>Jason: Enter exhibit one.</p> <p><img src="http://joellehman.com/cc/image_0.png" alt="image alt text"> *Photo of a white-board drawing during the CoordConv project, representing a possible pipeline for how painting shapes in particular coordinates could work in a neural network, suggesting a suite of experiments to narrow down what the challenging and easy parts of the learning problem were. *</p> <p>Jason: The whole pipeline is we need moving shapes generated from z (noise). It could involve a couple of steps. First from z (noise) to X and Y coordinates – that should be easy; any two dimensions in z could simply encode coordinate information. But can you from there produce reasonable logits for a single shape? And given reasonable logits can you produce a sharp 1-hot version, without softmax, just using convolutional layers? If you are painting two types of objects, I think we were gonna help the model by having two separate one-hots, one for the square, one for the circle. Assuming you had separate one hots, could you from there paint a circle and square?</p> <p>Jason: And then even if everything worked when supervised piecewise, could you go from X, Y all the way to painted shapes? Maybe every stage is trainable separately, but the whole thing end to end doesn’t work. So we started to prove whether this whole pipeline would or would not work, trained separately or together, with supervision or without. Then, this huge table was laid out; this is now early May, and T minus three weeks to NeurIPS, right?</p> <p><img src="http://joellehman.com/cc/image_1.png" alt="image alt text"> <em>Photo of a white-board diagram during the CoordConv project, suggesting a concrete set of experiments to explore what makes the painting problem difficult for deconvolutional networks, along with a nascent idea of what might provide a potential fix (CPPN).</em></p> <p>Jason: We had all these experiments – with soft max, without soft max, with deconv versus an improved network, looking at the different stages of a pixel-painting pipeline, with supervision, without supervision. There was a whole table of experiments to be run.</p> <p>Rosanne: And, I finished it.</p> <p>Jason: The whole table?</p> <p>Rosanne: Yeah! But we didn’t end up putting it in the paper. We reduced the story – a lot of it was just proving the concept. Those parts we thought should be easy, they were indeed easy. Not really surprising or interesting to show it in the paper.</p> <p>Jason: Right, right – given an input pixel, painting a square centered around it is easy, and nobody cares.</p> <p>Jason: We had this whole plan, from that picture. Which piece of the pipeline’s broken? That was a major step of the project. Something in this pipeline is broken. We just need to figure out which piece.</p> <p>Rosanne: Yes! Again the central question was whether I’m the failure or the model is the failure. The philosophical question of, is the world wrong, or am I wrong?</p> <p>Joel: So there’s the <em>problem</em> – but how did we arrive at the <em>solution</em>, what would become CoordConv? There was some kind of vague <a href="http://eplex.cs.ucf.edu/papers/stanley_gpem07.pdf" rel="external nofollow noopener" target="_blank">CPPN</a> inspiration, right?</p> <p>Jason: I think Joel’s idea was like we should just try putting a CPPN in this whole thing. And that was our goal, that was the thing we were going to try. I forget if we actually even tried that. I don’t know – or if we just realized, like why don’t we start with something simpler, which is just feeding in the coordinates.</p> <p>Rosanne: And I think we’ve got to CoordConv from CPPNs because I asked Joel how to implement a CPPN, and he said the easiest way is just to populate the coordinates as inputs to a 1x1 convolution – which got us thinking that well, we could just put coordinates as inputs into any convolution.</p> <h3 id="the-first-working-results">The First Working Results</h3> <p>Jason: So which result that made into the CoordConv paper did we get first? Was it single-pixel painting?</p> <p>Rosanne: The pixel painting and square painting were done at the same time. They’re very similar.We never had to tweak either one to make them work with CoordConv. That was working right away.</p> <p>Jason: And it was kind of shortly after that, or a bit in parallel, we tried the other direction too.</p> <p>Joel: Yeah, the coordinate regression. I think in all honesty, I think I had too much on my plate at the time, because I know I was really trying to get the models to work and I don’t remember the exact sequence of things, but, suddenly it started working with CoordConv. I could never get it to work without CoordConv, but I didn’t understand exactly what change helped CoordConv to begin to work. Once it started working I thought, well, I’m not going to mess with that any more, it works, that’s great. Then later it turned out that there was not a ReLU on the last layer, is that right?</p> <p>Rosanne: There’s not a ReLU on any layer – but max pooling at the end.</p> <p>Jason: So the version that first worked had no ReLUs? That’s interesting, it seems like we should’ve put that in the paper.</p> <p>Joel: Well, we didn’t know until afterwards – it’s in the code, but not in the paper. And then it turns out it’s sensitive to those ReLUs in some ways which aren’t quite obvious. I did some fiddling reimplementing it, and the model turned out to be more finicky than I’d like, although it works, and without CoordConv I could never get it to perform well.</p> <p>Jason: Cool – well that’s a <em>perfect</em> thing to include in the retrospective.</p> <p>Jason: So we had the pixel painting and the square painting, and then shortly after we had the inverse – the coordinate regression. At that point, which is about a few weeks before the paper deadline, we thought okay, this is the story. Something’s hard that you wouldn’t think was hard. This is why it’s hard – here’s a clear demonstration of it being hard, and you can make it easy by adding coordinates. Having the story is what gave us the ability to recruit other colleagues. Our team was the three of us plus Eric, and then we added three more people.</p> <p>Rosanne: Yes, that’s a good retrospective. That kind of team-building success I was not able to replicate consistently later on. I have since worked on projects with large and small teams, but it had never worked as nicely. With this one we got a team assembled in such a short time, because the solution was a simple function that you can easily wrap into a layer. Everyone plugged it into their framework and just tried it out, and it would work or not work, with really fast turn-around, as opposed to other projects when you try to incorporate more people towards the end, you give them this giant codebase that’s hard for them to catch up and figure out.</p> <p>Jason: Then what happened?</p> <p>Rosanne: We all just quickly tried things in parallel. We weren’t necessarily expecting anything – if there’s interesting results, we’d include them – but then they all turned out to be working.</p> <p>Jason: All right, so then we submitted to NeurIPS. It was a rush at the last minute, but we got it out there.</p> <h3 id="making-the-companion-video">Making the Companion Video</h3> <p>Joel: So when did <a href="https://www.youtube.com/watch?v=8yFQc6elePA" rel="external nofollow noopener" target="_blank">the video</a> happen – was that after we knew it was accepted?</p> <p>Rosanne: That was in July – no, we didn’t know the acceptance yet. We tried to get the video out before ICML, that was our strategy. ICML happened before NeurIPS acceptance notices went out.</p> <p>Jason: We spent a month, getting the clean version of the arXiv version out, which was much better than the NeurIPS submission, and writing the <a href="https://eng.uber.com/coordconv/" rel="external nofollow noopener" target="_blank">blog post</a>. And the video took a whole weekend to make….which was…<em>fun?</em></p> <p>Joel: It was good, it was intense.</p> <p>Rosanne: We should do it again sometime, when we have another paper with Joel, to feature his bad acting.</p> <p>Joel: Corn Conv, Corn cob, whatever! It’s hard to pronounce.</p> <p>[laughter]</p> <p>Rosanne: Well the whole video to film didn’t take much more than a weekend.</p> <p>Jason: It was about two or three days of making the script, which we also finished during the weekend. And then a solid weekend filming, and then editing afterwards, it does take a long time. Maybe there are better tools than iMovie, or better people than me, but it’s not fast at all.</p> <p>Rosanne: Is that a retrospective part – that we could have hired professionals to help make it? But it seems cute, how it was made by non-professionals, and it’s more to our taste.</p> <p>Rosanne: In general, I think the video is a very good idea, it was Jason and I’s second time doing it together, and Jason has done more before – I think people know us more because of it.</p> <p>Jason: I think the idea is that if you have N minutes of someone’s time, pack as much information, actual knowledge transfer in there as you can, while making it fun enough that they don’t close the video. I think that’s what we aimed at, and we did a decent job at that.</p> <p>Rosanne: You’ve always had a style of communicating to people about your ideas, in a simple way. You write “casual abstracts,” for your papers, before the whole video shooting idea. The video idea was quite a success, but what I’m saying is it’s not an accident. It naturally came about with our way of doing things.</p> <p>Jason: I didn’t use to write those casual abstracts. Then for one paper, I posted on Facebook, we have this paper, and then added a comment, <em>here’s a short casual description of what we’re doing in plain English</em>. Then the next paper I posted, someone’s first comment was “Where’s the casual abstract, capital C, capital A, parentheses, T.M.?” So now I capitalize it.</p> <p>Jason: One thing I realized once ICLR started and they were using open interview, was that since the reviews are public, you get to see reviewers answer the question for every paper “summarize the paper simply.” What I realized is that reading someone’s summary of the paper is so much better than reading either the <em>abstract</em> or the <em>paper</em>. Why is it so much better than the paper’s abstract – because it’s like debiased and de-hyped. It’s just, here’s what they actually did. It’s so useful. You feel like you understand almost the whole paper, by reading an honest description of it.</p> <p>Joel: It’s without all the framing and strategy, to maximize chances at acceptance or impact.</p> <p>Jason: Yeah, without overstating or claiming over-generalization of this big idea, which you didn’t really execute on.</p> <p>Rosanne: Without all that (the framing and strategy angles), it’s just a language switch – from formal to casual, which papers somehow still don’t allow.</p> <h3 id="what-you-would-do-differently">What You Would Do Differently</h3> <p>Joel: One prompt for retrospectives is, if you were telling a friend about this paper, how would you tell it differently to them than how it is in the paper?</p> <p>Rosanne: I would tell the story backwards, almost – in the paper it’s “we found this failure, we’re digging into this failure, and then we’re half-way in the paper, and then only one-fifth of the paper is about how good the fix is and how it works” – if I were to tell someone in person I might just say, I have a fix for Conv, works well almost everywhere, if you want to know more about how we discovered it, I can tell you.</p> <p>Joel: Yeah, it’s just a couple of lines of code, and if there’s some intuition you have that your problem or model could use a geometric bias, why not give it a shot?</p> <p>Jason: I think my elevator version is, you would think it’s easy to make convolution paint some pictures, for pictures, for example, a single pixel – you tell it what pixel. Our paper shows it’s actually really hard.</p> <p>Jason: I think one thing, it’s not a difference in how the paper presented it and I would present it now, but it’s the difference between how the paper was received and how I wish it was received. People have a strong bias thinking that all papers have to be about how we invented something new that’s better, rather than here’s a distillation of knowledge that you should probably have. I think our paper’s most important part is the discovery of this problem, the rigorous measuring and explanation for what’s going on. And then the fix is almost irrelevant, there are other possible fixes. Although, we did make the name of the paper contain CoordConv. There were so many people that read it, reviewers and others, saying “People have tried this before, this isn’t that new.” But we weren’t trying to say that – that’s not the interesting part. That’s a gap between what we intended and the way it was received – so what could we do differently next time to decrease that gap? We could’ve not named the fix. We could’ve maybe given a stronger name to the problem itself. We did call it the coordinate transform problem, but it wasn’t in the title.</p> <p>Jason: Also, we definitely could have cited more papers in the initial version. We only found out about all the other similar methods after we put the work out there. People yelled at us, then we added a lot of cites. I wish we could have narrowed that gap.</p> <p>Rosanne: Somehow I think naming was a good idea, it was a good idea for selling the paper. I mean, selling and branding is part of what we do, it’s important.</p> <p>Jason: True – even with the angry people, I don’t think we should regret the name, because it really worked. Another question is, do you regret not releasing the code earlier? Another happy accident!</p> <p>Rosanne: Yeah – a happy accident! Now, I regret it less. I used to regret it more. I released the code almost 7 months later than I should, and during that time there were numerous implementations that people did for me. Happy accident, because I indirectly encouraged the community to do this thing that I didn’t do.</p> <p>Rosanne: Also – the first release of the paper, the sample code we provided in the appendix (it’s simple enough to be in a few lines), had a little bug. Someone found that, and they released their version of the code – they said, CoordConv with bug fixed, and people would star that implementation.</p> <p>Jason: The version you eventually released, you fixed that bug?</p> <p>Rosanne: Yeah, of course – it was just a bug that if you used a different height and width then it would break, but we used square images in all our experiments, so it didn’t come up for us.</p> <h2 id="new-perspectives">New Perspectives</h2> <p>Jason: One question is, how do you see the paper differently now than back then – new perspectives on your own work?</p> <p>Joel: I don’t know that I see the work itself in a different light, but the process by which that work came to be is interesting to me, how research happens, how important it is to always be curious. If you find an unexpected problem, then drill into that problem. It’s inspiring, how Rosanne kept going and figured this out.</p> <p>Rosanne: I feel there was a big chance the paper would go without the fix. We knew we would find the problem, but we didn’t know we would find the fix. Revealing a problem itself seems like a worthwhile scientific investigation, but it wouldn’t be as good of a paper to the community. Is having that kind of good paper related to having the luck to find something that also has a fix?</p> <p>Jason: I think most times when you really understand a problem, a fix can often be obvious. I think the hard problem ends up being understanding the problem.</p> <p>Joel: And how do you look at it differently now?</p> <p>Jason: I’d say the science is just as it was, I don’t see it differently now. The process was interesting. The process worked because we were as fast as we were. If we took our process and slowed it down by a factor of two, by the deadline we would have had nothing at all, and we would have killed the project, because at that point it was just GANs that don’t work, right, which aren’t that interesting. The way the team came together, too, was important, so the fact that the four of us (with Eric) were pretty committed early on, and worked on it for quite a while, put in enough time to see it through. Then we added people on at the end to hammer the point home, we showed that it does generalize quite a bit.</p> <p>Rosanne: I think that’s because we had experience working together – a year before we worked together on another project, code-named LUprop, which sadly didn’t make it to NeurIPS. But we had gone through the research process together, especially a multitude of difficulties together, and so we had the faith that this time we can make something, or even if not we’d have fun. Because like most research, there are lots of moments we could have given up, when things just were not working. So trust-worthy work companions makes a big difference.</p> <p>Jason: You trust enough, you’re fast enough, you’re good enough, you like each other enough. That way, when you hit a bit of a wall, you can turn, and roll with that. There were several turns that could have killed the project without a team that was as well-equipped.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 ICBD Workshop. IEEE International Conference on Big Data 2024 Workshop </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/icbd2024/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/icbd2024/assets/js/masonry.js" type="text/javascript"></script> <script src="/icbd2024/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/icbd2024/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/icbd2024/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/icbd2024/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/icbd2024/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script> </script> </body> </html>